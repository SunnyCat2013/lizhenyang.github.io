# Connectionist Temporal Classification

> 因为最近做了一些用连续标签做文字识别标签任务的工作，对 ctc 有了一些了解，在此记录一下。

在学习 CTC 的时候，也看了不少博客，但是我觉得讲的最好的还是原论文 [Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.75.6306&rep=rep1&type=pdf) 解释的最清楚。  
对于没接触过这个概念的人，可能加一些例子会更好理解一些。  
我就来加一些例子。

# 背景知识

用实例来说，我们在做 ocr 工作时，我们希望给一行文字的图片让机器识别出来这个图片里面的文字。  
语音识别任务中，给了一段语音片段，我们希望能把这段语音识别成可编辑的文字。  
但是，在对每个片段进行分类模型训练之前，需要对每个训练样本进行切割标注。  
这是项非常繁琐的工作非常不利于模型的训练。

下面是个对文字进行标注的工具，大家可以看一下。如果我们在做文字识别工作时，对每个文字都要明确标出这个字在图片中的位置、高、宽，这将会是一个多么巨大的工作量。

![jtessboxeditor](./jTessBoxEditor.png)

RNNs 在序列学习任务中有着优越的性能，但是它也有一些缺点。  
如，在上面描述的那种对输入模型的数据需要预处理的缺点。同时，对 RNNs 的序列也需要一定的整合才能得到最终的预测序列。  
而 CTC 解决了对输入序列的单个词的切分和对输入序列的整合工作。

# RNN 的输出

输入序列的切分与标注上面已经举了一个例子，现在举一个输出序列整合的例子。  
我们现在有一个图片的输入 ![hello](./hello.png)。  
假设这个图片中每个红都作为 RNN 的一步输入，那么（如果这个模型训练的还不错的话）它的输出序列应该是 `hheelloo`。  
但是，我们知道 RNN 每一步的输出其实都是一组概率分布，$$p(l|x), l \in Alphebat$$。  
如，对第一个矩形框的输出概率可能是 $$p(l = 'h' | x) = 0.5, p(l = 'm' | x) = 0.3 \cdots$$

# 时序分类（Temporal Classification）

先给几个定义。

| 符号 | 解释 |
| --- | --- |
|　$$L$$ | 字母表 `Alphebat`|
| $$ (R^m)^\ast $$ | $$m$$ 表示输入数据的一个“宽度”，如我们输入的是一个图片时，宽度可以是一个定值。 $$\ast$$ 表示这个串的长度不定 $$\in [0, +\infty)$$，如输入图片的长度是未知长度。|
| $$\chi$$ | $$\chi = (R^m)^\ast$$ 表示输入数据空间。 |
| $$Z$$ | $$Z = L^\ast$$ 表示由字母表排列而成的标签集合，我们可以理解成单词表。 |
| $$D_{\chi \times Z}$$ | 真实数据空间。|
| `z` | `z` $$ = (z_1, z_2, \dots, z_U)$$  是 $$Z$$ 中的一个样本。 |
| `x` | `x` $$ = (x_1, x_2, \dots, x_T), U \leq T$$，表示一个样本输入数据的输入序列。如一个定高图片每一列像素可以认为是一个 $$x_i$$。|
| $$S$$ | $$S \subset D_{\chi \times Z}$$ 训练样本集，这个集合中的每个样本都是一个 `(x, z)` 组合 |
| $$S'$$ | $$S' \subset D_{\chi \times Z}, S' \bigcap S = \emptyset$$ |
| $$h$$ | 时序分类器。|

由上面的定义，我们可以看出，因为输入和输出和长度未必相等，所以没有办法事情把这两种数据对齐。
## 目标
时序分类的目标就是学习
$$
h: \chi \longmapsto Z
$$

## 损失函数

用于 CTC 的损失函数是 `Lebal Error Rate(LER)`。
这里我们需要知道“[最小编辑距离（Edit Distance, ED）](https://en.wikipedia.org/wiki/Edit_distance)”这个概念，在 CTC 的损失函数就用到了。 
> 在学习算法的时候，`ED` 算是一个比较经典的动态规划问题，但在实际工作中其实很少用到这类算法。
所以第一次知道这个算法能用在这里，我还是挺开心的。

损失函数定义如下：
$$
LER(h, S') = \frac{1}{|S'|}\sum_{(x,z) \in S'}\frac{ED(h(x), z)}{|z|}
$$
用编辑距离（ED）来衡量文字串的预测情况还是一件蛮符合直观理解的事情。


# 连接的时序分类（Connectionist Temporal Classification）
写了半天终于到正题了，下面开始讲 CTC!
CTC 网络的 `softmax` 输出层输出的类别有 $$|L| + 1$$ 种，因为有一个分隔符，比如说是空格。
> 这个分隔符其实蛮重要的，它可以很好地区分一个输出序列串中，哪些子串是属于同一个文字的图片区域的输出结果。

## 一个输出序列的概率
首先，我们来看一个实例：输入 `x` 的长度是 `T`，每一个帧的维度是 `m`。模型的输出的长度也是 `T`，每一帧的维度是 `n`。其中 `m` `n` 可以相同也可以不同。用数学定义我们的这个模型就是：

$$
N_w: (R^m)^T \longmapsto (R^n)^T
$$
